---
title: "Loan and Deposit ARIMA Forecasting"
author: "Thomas Pfeiffer"
date: "7/15/2016"
output: pdf_document
---

```{r, echo=FALSE, message=FALSE}
cdata <-read.csv("/Users/ThomasPfeiffer/Documents/R Programming/Cobelen/InformeFinanciero_Cobelen.csv", 
                        stringsAsFactors=FALSE, header=TRUE)
```

##Introduction
  This document is a summary of a data analysis process from raw data to finished forecasts. The data included monthly loan and deposit data for forty-five from twelve branches of a cooperative. The task was to create forecasts, for each branch and for the total cooperative, for the coming year, 2017. Before beginning it is very important to note that forecasting eighteen months in advance is likely to lead to very innacurate forecasts, and although this is understood and acknowledged, the task does not change. 
  Below you will find the process that was used to create forecasts for this data set.

##Functions and Packages Needed:
 Here are the packages that will be used during this analysis with explanations for their uses:
```{r, message=FALSE}
library(ggplot2) # For creating graphs
library(gridExtra) # Use ggplot2 with multiple graphs on one plot
library(forecast) # Create forecasts
library(tseries) # Time series analysis
library(corrplot) # Create a correlation matrix
library(tidyr) # allows for the gather() function that makes "wide" data "long"
```

Here is a function that was used plot and evaluate normality:

```{r, message=FALSE}
plotForecastErrors <- function(forecasterrors)
{
        # make a histogram of the forecast errors:
        mybinsize <- IQR(forecasterrors)/4
        mysd <- sd(forecasterrors)
        mymin <- min(forecasterrors) - mysd*4
        mymax <- max(forecasterrors) + mysd*4
        # generate normally distributed data with mean 0 and standard deviation mysd
        mynorm <- rnorm(10000, mean=0, sd=mysd)
        mymin2 <- min(mynorm)
        mymax2 <- max(mynorm)
        if (mymin2 < mymin) { mymin <- mymin2 }
        if (mymax2 > mymax) { mymax <- mymax2 }
        # make a red histogram of the forecast errors, with the normally distributed data overlaid:
        mybins <- seq(mymin, mymax, mybinsize)
        hist(forecasterrors, col="red", freq=FALSE, breaks=mybins)
        # freq=FALSE ensures the area under the histogram = 1
        # generate normally distributed data with mean 0 and standard deviation mysd
        myhist <- hist(mynorm, plot=FALSE, breaks=mybins)
        # plot the normal curve as a blue line on top of the histogram of forecast errors:
        points(myhist$mids, myhist$density, type="l", col="blue", lwd=2)
}
```


##Data
After importing the raw data, here is how the data were formatted so that every column is its own time series:

```{r}
cdata <- cdata[, c("Agencia", "Fecha", "SaldoCartera", "SaldoCDAT")]
cdata[,-1:-2] <- sapply(cdata[,-1:-2], function(x)as.numeric(gsub(",", "", x)))
cdata$Agencia <- as.factor(cdata$Agencia)
cdata$Fecha <- as.Date(cdata$Fecha, format = "%m/%d/%y")
#Organize data into 26 time series
data <- setNames(gather(cdata, "cd_or_loan", "values", 3:4), c('branch','date','cd_or_loan','values'))
data[,4] <- data[,4] / 1000000 # In millions for ease of graphing and forecasting
data <- spread(data, branch, values)
loan <- data[data$cd_or_loan == "SaldoCartera",-2]
cd <- data[data$cd_or_loan == "SaldoCDAT",-2]
loan <- setNames(loan, c('date','1l','2l','3l','4l','5l','6l','7l','8l','9l','10l','11l','12l'))
cd <- setNames(cd, c('date','1cd','2cd','3cd','4cd','5cd','6cd','7cd','8cd','9cd','10cd','11cd','12cd'))
data <- cbind(loan, cd[,-1])
data$totall <- apply(loan[,-1], MARGIN = 1, sum) #create loan and cd totals
data$totalcd <- apply(cd[,-1], MARGIN = 1, sum)
```

####Plotting the Data
```{r}
totals <- setNames(data.frame(rep("T", 45), data$date, data$totall, data$totalcd), 
                   c('Agencia', 'Fecha', 'SaldoCartera', 'SaldoCDAT'))
gdata <- gather(rbind(cdata, totals), "cd_or_loan", "values", 3:4) # Data format for graphing
totals <- gather(totals, "cd_or_loan", "values", 3:4)
ggplot(totals, aes(Fecha, values, col=factor(cd_or_loan, labels = c("Loans", "CD")))) + geom_line() +
        labs(title="Total Balances", x="Date", y="Balance (millions)", color="CD or Loan") +
        theme(plot.title=element_text(size=rel(2), hjust=0.5 ), legend.key=element_rect(color="black"))
ggplot(cdata, aes(Fecha, SaldoCDAT, color=Agencia)) + geom_line() + 
        labs(title="CD Balances by Branch", x="Date", y="CD Balance (millions)", color="Branch") +
        theme(plot.title=element_text(size=rel(2), hjust=0.5), legend.key=element_rect(color="black")) +
        scale_y_continuous(labels = function(x)x/1000000)
ggplot(cdata, aes(Fecha, SaldoCartera, color = Agencia)) + geom_line() + 
        labs(title="Loan Balances by Branch", x="Date", y="Loan Balance (millions)", color="Branch") +
        theme(plot.title=element_text(size=rel(2), hjust=0.5), legend.key=element_rect(color="black")) +
        scale_y_continuous(labels=function(x)x/1000000)
```

From our main graph, we can see that all branches have a positive trend in both CD and Loan balances, except for Branch 9.
It is also interesting to note that (possibly because of our limited data) there is no apparent seasonal factor in the data.

Here are the graphs for all twelve individual branches:

```{r}
for(i in 1:12) {
g <- ggplot(gdata[gdata$Agencia==i,], aes(Fecha, values, color=factor(cd_or_loan,labels=c("Loans", "CD"))))+
        geom_line() + 
        labs(title= paste("Branch ", i, sep=""), x="Date", y="Balance (millions)", color="CD or Loan") + 
        theme(plot.title=element_text(size=rel(2), hjust=0.5), legend.key=element_rect(color="black")) + 
        scale_y_continuous(labels=function(x)x/1000000)
assign(paste("b", i, "_graph", sep=""), g)
}
grid.arrange(b1_graph, b2_graph, b3_graph, b4_graph, b5_graph, b6_graph, ncol=2)
grid.arrange(b7_graph, b8_graph, b9_graph, b10_graph, b11_graph, b12_graph, ncol=2)
```

##Forecasting

  In order to forecast the data we will implement an ARIMA model. ARIMA has been chosen over other methods because the main predictor for future performance is mainly previous data. Furthermore, ARIMA was chosen over exponential smoothing because ARIMA is considered simpler, and exponential smoothing models can be restated as ARIMA models ( i.e. additive exponential smoothing models have ARIMA counterparts, and multiplicative models could be expressed as ARIMA models on a logged series).
  The ARIMA process uses regression/correlation statistics to identify the stochastic patterns in the data. Once we have a stationary time series, we must ask two questions:
  
1. Does the data show an AR or MA process?
  * To answer this, we must look at autocorrelation fo the data
  * AR model is represented by a spike in autocorrelation followed by a gradual decrease
    + AR formulation: x(t) = alpha * x(t-1) + error(t)
  *In MA models, the autocorrelation quickly drops off, not gradually like the AR model
    + MA formulation: x(t) = beta * error(t-1) + error(t)
2. What order of AR or MA process do we need to use?
  * Look at the periods of correlation to determine.
  
####MAKE DATA STATIONARY
  In order to forecast using ARIMA, the data should be stationary – a stationary time series is one whose statistical properties such as mean, variance, etc. are constant over time. Differencing means taking the difference between consecutive observations, or between observations a year apart: y(t) - y(t-1). A stationary time series is one whose statistical properties such as mean, variance, etc. are constant over time. The mean can not change over time (trend), the variance can not change over time (spread). Unit root tests for stationarity: KPSS, Augmented Dickey-Fuller, Phillips-Perron
  
####ARIMA
  Forecasting in R is quite simple due to the packages and work that has already been done. In order to forecast using ARIMA, the `auto.arima()` funcion will be used from the `forecast` package, and this method will be accepted as long as the error analysis is satisfactory. `auto.arima()` returns best ARIMA model according to either AIC, AICc or BIC value among the possible arima models.
  An ARIMA model is usually stated as ARIMA(p,d,q). This represents the order of the autoregressive components (p), the number of differencing operators (d), and the highest order of the moving average term (q).
  * AR - previous _y values_ determine future y values
  * MA - previous _error terms_ determine future y values

  Although the forecast after many periods in the future are likely very inaccurate, with this acknowledged, the forecast will be generated for the next 18 periods (months).
  ARIMA total loan and deposit forecasts:

```{r}
totall_arima <- auto.arima(data$totall)
arimaorder(totall_arima)
totalcd_arima <- auto.arima(data$totalcd)
arimaorder(totalcd_arima)
totall_fcast <- forecast(totall_arima, 18)
totalcd_fcast <- forecast(totalcd_arima, 18)
plot(totall_fcast, type="o", ylab="Loan Balance (millions)", main="Total Loan ARIMA Forecast")
plot(totalcd_fcast, type="o", ylab="CD Balance (millions)", main="Total CD ARIMA Forecast")
```
  
  We can see that the models chosen from auto.arima for both totals is ARIMA(0,1,1), meaning that there is a single differencing and that it is a moving average model. This is equivalent to a Simple Exponential Smoothing model. 
  The forecasting equation for this model is Ŷt = μ + Yt-1 - θ(1)*e(t-1)

  
  To check to see that this makes sense, we can look at the correlation plots (acf and pacf) after differencing:
  
```{r}
nl <- ndiffs(data$totall, alpha = 0.05, test = c("kpss", "adf", "pp"))
ncd <- ndiffs(data$totalcd, alpha = 0.05, test = c("kpss", "adf", "pp"))
ddatal <- diff(data$totall, differences = nl) 
ddatacd <- diff(data$totalcd, differences = ncd)
par(mfrow = c(1,2))
acf(ddatal, lag.max = 25, main = "") #correlation of y(t) & y(t-n)
pacf(ddatal, lag.max = 25, main = "") #correlation of y(t) & y(t-n) after removing other time lag effects
acf(ddatacd, lag.max = 25, main = "")
pacf(ddatacd, lag.max = 25, main = "")
dev.off()
```
  
  From the correlation plots, we can see that there isn't much correlation among the differenced data, and since it appears to be essentially random with a trend, a simple moving average model does appear to be best.
  
####Error Analysis
  We want to check the errors of our model for some constraints: normality, independence, constant variance, and error mean of zero.
  
* Normality - required if the confidence intervals are to be of any use
* Independence - an accurate model will use all of the correlation within the data, and if after forecasting there is still correlation among the errors, that means that a parameter should be added so that the model is more accurate.
* Constant variance - heteroscedasticity means that the model is unable to explain some pattern in the response variable (Y), that eventually shows up in the residuals. This would result in an inconsistent and unstable model that could yield bizarre predictions later on. Thus, the data must be re-evaluated and possibly transformed (such as square roots or logarithms) in order to stabilize the variance. 
* Error mean of zero - if the error mean is not very near to zero then there is clearly an inconsistency in your model

```{r}
#1-Normality
par(mfrow=c(1,2))
qqnorm(totall_fcast$residuals); qqline(totall_fcast$residuals, col = 2) #qqplots
plotForecastErrors(totall_fcast$residuals) #distribution histograms
qqnorm(totalcd_fcast$residuals); qqline(totalcd_fcast$residuals, col = 2)
plotForecastErrors(totalcd_fcast$residuals)
```

  Normality analysis shows that the errors for both models are approximately normal. Therefore, it is plausible to use the confidence intervals that are produced from the forecast, however, the normality is tentative and the confidence intervals should not be depended upon for reliability. 
  

```{r}
#2-Independence (i.e. zero autocorrelations)
Box.test(totall_fcast$residuals, lag=20, type="Ljung-Box") #H0: Erors are independent
Box.test(totalcd_fcast$residuals, lag=20, type="Ljung-Box") #H1: Errors are correlated
```

  The Ljung-Box tests for independence of the errors concludes that we cannot reject the null hypothesis that the errors are independent. Therefore, it appears that our ARIMA model uses a substantial amount of the predictable information available in the data.
  

```{r}
#3-Constant Variance & Mean=Zero
par(mfrow=c(1,2))
plot(totall_fcast$residuals)
acf(totall_fcast$residuals)
plot(totalcd_fcast$residuals)
acf(totalcd_fcast$residuals)
```

  It appears that the errors for each forecast are possibly heteroscedastic, however, they are consistent enough to be satisfactory. Also, the residual mean is very near to zero.
  From the error analysis, it appears that the errors meet all the requirements to satisfy the reliability of our forecasts. Therefore, we have successfully forecasted the total loan and total deposit balances. 
  

####Forecasts for remaining time series

  Now it is necessary to create ARIMA forecasts for both loan and deposit balances for the 12 branches. In order to do this efficiently, a similar process will be applied to all branches, and the errors will then be checked for problems. Although it may be possible to implement a function and loop or apply over all of the data, because each model should be individually made and certain time series need transformations, it seems most appropriate to create the forecasts individually for the twelve branches. To save space on this page, each process will not be described, and instead the final forecasts will be presented graphically and numerically below:


```{r, echo=FALSE}
data[1:7,10] <- 2605.949 # Remove outliers from branch nine 
par(mfrow=c(1,2))
for(i in 2:13) {
        la <- auto.arima(data[,i])
        cda <- auto.arima(data[,i+12])
        lfc <- forecast(la, 18)
        cdfc <- forecast(cda, 18)
        #PLOT
        plot(lfc,type="o",ylab="Loan Balance (millions)",main=paste("B",i-1,"Loan ARIMA Forecast",sep=''))
        plot(cdfc,type="o",ylab="CD Balance (millions)",main=paste("B",i-1,"CD ARIMA Forecast",sep=''))
}
```
